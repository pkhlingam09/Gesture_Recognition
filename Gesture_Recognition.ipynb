{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_27lvKC0w9J"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JfFJg07qe4IL",
        "outputId": "351f91f8-edb2-4b60-ff44-c3a3b1ff0767"
      },
      "outputs": [],
      "source": [
        "def download_Project():\n",
        "  !pip install gdown\n",
        "  !gdown https://drive.google.com/uc?id=1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL\n",
        "  !unzip /content/Project_data.zip\n",
        "\n",
        "download_Project()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IvS1Cin7WuK4"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random as rn\n",
        "import numpy.random as nrn\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# import keras\n",
        "# from keras.models import Sequential, Model\n",
        "# from keras import layers\n",
        "# from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "# from keras.layers import Conv3D, MaxPooling3D, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
        "# from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras import optimizers\n",
        "\n",
        "from keras.applications import ResNet101\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ba_U1A1XWuIN"
      },
      "outputs": [],
      "source": [
        "rn.seed(37)\n",
        "nrn.seed(37)\n",
        "tf.random.set_seed(37)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mho_Mhf8WuF2"
      },
      "outputs": [],
      "source": [
        "data_folder = \"E:/Deep Learning/Gesture Recognition/Project_data\"\n",
        "\n",
        "train_df = pd.read_csv(f\"{data_folder}/train.csv\", sep=\";\", header=None)\n",
        "train_df.columns = ['folder', 'type', 'class']\n",
        "val_df = pd.read_csv(f\"{data_folder}/val.csv\", sep=\";\", header=None)\n",
        "val_df.columns = ['folder', 'type', 'class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "L4YVBUgFWuDN"
      },
      "outputs": [],
      "source": [
        "def get_folder_list(df_csv):\n",
        "  fptr = open(df_csv)\n",
        "  folder_paths = fptr.readlines()\n",
        "  fptr.close()\n",
        "  return folder_paths\n",
        "\n",
        "def get_batch_details(folder_list, batch_size):\n",
        "  num_batches = len(folder_list) // batch_size\n",
        "  folders_remain = len(folder_list) - (num_batches * batch_size)\n",
        "  return num_batches, folders_remain\n",
        "\n",
        "# def create_batch_folders(batch_size, img_ids, image_shape):\n",
        "#          ## batch_images                                                                               ## batch_labels\n",
        "#   return (np.zeros(shape = (batch_size, len(img_ids), image_shape[0], image_shape[1], 3)), np.zeros(shape = (batch_size, 5)))\n",
        "\n",
        "def process_images(folder, folder_path, img_ids, image_shape, batch_images):\n",
        "  imgs = os.listdir(f'{folder_path.split(\";\")[0]}')\n",
        "  for idx, ind in enumerate(img_ids):\n",
        "    ## read image\n",
        "    image = cv2.imread(f'{folder_path.split(\";\")[0]}/{imgs[ind]}').astype(np.float32)\n",
        "    ## resize image\n",
        "    image = cv2.resize(image, (image_shape[0], image_shape[1]), interpolation = cv2.INTER_LINEAR)\n",
        "    ## Seperating them into their respective R, G, B channels and normalizing the values\n",
        "    ## Load labels for each image\n",
        "    batch_images[folder, idx, :, :, 0] = image[:,:,0]/255.0   ## R\n",
        "    batch_images[folder, idx, :, :, 1] = image[:,:,1]/255.0   ## G\n",
        "    batch_images[folder, idx, :, :, 2] = image[:,:,2]/255.0   ## B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## path => folder path as string\n",
        "## folder_list => list of folders in train/test folder as list\n",
        "## batch_size => chosen batch size as int\n",
        "## image_shape => as tuple (image_height, image_width)\n",
        "def run_generator(path, folder_list, batch_size, image_shape):\n",
        "  num_batches, folders_remain = get_batch_details(folder_list, batch_size)\n",
        "  ## set how many images to view from total images of the video\n",
        "  img_ids = list(range(0,30,2))\n",
        "  while True:\n",
        "    folder_list = np.random.permutation(folder_list)\n",
        "    for batch in range(num_batches):\n",
        "      # batch_images, batch_labels = create_batch_folders(batch_size, img_ids, image_shape)\n",
        "      batch_images = np.zeros(shape = (batch_size, len(img_ids), image_shape[0], image_shape[1], 3))\n",
        "      batch_labels = np.zeros(shape = (batch_size, 5))\n",
        "      ## get folders for that batch\n",
        "      for folder in range(batch_size):\n",
        "        # process_images(folder, f'{path}/{folder_list[folder + (batch * batch_size)]}', img_ids, image_shape, batch_images)\n",
        "      ## get images and labels for each folder in the batch\n",
        "        imgs = os.listdir(f'{path}/{folder_list[folder + (batch * batch_size)].split(\";\")[0]}')\n",
        "        for idx, ind in enumerate(img_ids):\n",
        "            ## read image\n",
        "            image = cv2.imread(f'{path}/{folder_list[folder + (batch * batch_size)].split(\";\")[0]}/{imgs[ind]}').astype(np.float32)\n",
        "            ## resize image\n",
        "            image = cv2.resize(image, (image_shape[0], image_shape[1]), interpolation = cv2.INTER_LINEAR)\n",
        "            ## Seperating them into their respective R, G, B channels and normalizing the values\n",
        "            ## Load labels for each image\n",
        "            batch_images[folder, idx, :, :, 0] = image[:,:,0]/255.0   ## R\n",
        "            batch_images[folder, idx, :, :, 1] = image[:,:,1]/255.0   ## G\n",
        "            batch_images[folder, idx, :, :, 2] = image[:,:,2]/255.0   ## B\n",
        "          ## Load labels for each image\n",
        "        batch_labels[folder, int(folder_list[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
        "      yield batch_images, batch_labels\n",
        "      \n",
        "    ## Process the remaining folders\n",
        "    # batch_images, batch_labels = create_batch_folders(folders_remain, img_ids, image_shape)\n",
        "    if folders_remain > 0:\n",
        "      batch_images = np.zeros(shape = (folders_remain, len(img_ids), image_shape[0], image_shape[1], 3))\n",
        "      batch_labels = np.zeros(shape = (folders_remain, 5))\n",
        "      folder_rem = folder_list[-folders_remain:]\n",
        "      for folder in range(folders_remain):\n",
        "      #   process_images(folder, f'{path}/{folder_rem[folder]}', img_ids, image_shape, batch_images)\n",
        "        imgs = os.listdir(f'{path}/{folder_rem[folder].split(\";\")[0]}')\n",
        "        for idx, ind in enumerate(img_ids):\n",
        "            ## read image\n",
        "            image = cv2.imread(f'{path}/{folder_rem[folder].split(\";\")[0]}/{imgs[ind]}').astype(np.float32)\n",
        "            ## resize image\n",
        "            image = cv2.resize(image, (image_shape[0], image_shape[1]), interpolation = cv2.INTER_LINEAR)\n",
        "            ## Seperating them into their respective R, G, B channels and normalizing the values\n",
        "            ## Load labels for each image\n",
        "            batch_images[folder, idx, :, :, 0] = image[:,:,0]/255.0   ## R\n",
        "            batch_images[folder, idx, :, :, 1] = image[:,:,1]/255.0   ## G\n",
        "            batch_images[folder, idx, :, :, 2] = image[:,:,2]/255.0   ## B\n",
        "        batch_labels[folder, int(folder_rem[folder].strip().split(';')[2])] = 1\n",
        "      yield batch_images, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6fCoXkFJWuAw"
      },
      "outputs": [],
      "source": [
        "# ## path => folder path as string\n",
        "# ## folder_list => list of folders in train/test folder as list\n",
        "# ## batch_size => chosen batch size as int\n",
        "# ## image_shape => as tuple (image_height, image_width)\n",
        "# def run_generator(path, folder_list, batch_size, image_shape):\n",
        "#   num_batches, folders_remain = get_batch_details(folder_list, batch_size)\n",
        "#   ## set how many images to view from total images of the video\n",
        "#   img_ids = list(range(0,30,2))\n",
        "#   while True:\n",
        "#     folder_list = np.random.permutation(folder_list)\n",
        "#     for batch in range(num_batches):\n",
        "#       batch_images, batch_labels = create_batch_folders(batch_size, img_ids, image_shape)\n",
        "#       ## get folders for that batch\n",
        "#       for folder in range(batch_size):\n",
        "#         process_images(folder, f'{path}/{folder_list[folder + (batch * batch_size)]}', img_ids, image_shape, batch_images)\n",
        "#       ## get images and labels for each folder in the batch\n",
        "#       #####################################\n",
        "#           ## Load labels for each image\n",
        "#         batch_labels[folder, int(folder_list[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
        "#       yield batch_images, batch_labels\n",
        "\n",
        "#     ## Process the remaining folders\n",
        "#     batch_images, batch_labels = create_batch_folders(folders_remain, img_ids, image_shape)\n",
        "#     folder_rem = folder_list[-folders_remain:]\n",
        "#     for folder in range(folders_remain):\n",
        "#       process_images(folder, f'{path}/{folder_rem[folder]}', img_ids, image_shape, batch_images)\n",
        "#       ####################################\n",
        "#       batch_labels[folder, int(folder_rem[folder].strip().split(';')[2])] = 1\n",
        "#     yield batch_images, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QUU21eKyb0RB"
      },
      "outputs": [],
      "source": [
        "curr_dt_time = datetime.now()\n",
        "\n",
        "model_name = 'model_cnn3D' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.keras'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto')\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_lr=0.001)\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O3QJSl88Wt-o"
      },
      "outputs": [],
      "source": [
        "# train_folders = []\n",
        "# val_folders = []\n",
        "# for i in range(5):\n",
        "#   train_folders.extend(train_df.loc[train_df['class'] == i, ['folder', 'class']][:20])\n",
        "#   val_folders.extend(train_df.loc[train_df['class'] == i, ['folder', 'class']][-5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cTKf4vAAUsMO"
      },
      "outputs": [],
      "source": [
        "fptr = open(f'{data_folder}/train.csv')\n",
        "train_paths = fptr.readlines()\n",
        "fptr.close()\n",
        "train_folder_list_ = np.random.permutation(train_paths)\n",
        "\n",
        "\n",
        "fptr = open(f'{data_folder}/val.csv')\n",
        "val_paths = fptr.readlines()\n",
        "fptr.close()\n",
        "val_folder_list_ = np.random.permutation(val_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BeZMgFwmhILY"
      },
      "outputs": [],
      "source": [
        "train_folder_list = train_folder_list_[:150]\n",
        "val_folder_list = val_folder_list_[-50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "32Z2BAr4Wt8U"
      },
      "outputs": [],
      "source": [
        "# train_folder_list = np.random.permutation(train_folder_list)\n",
        "# val_folder_list = np.random.permutation(val_folder_list)\n",
        "# len(train_folder_list), len(val_folder_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CUFK0doAfyi0"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "num_epochs = 20\n",
        "image_shape = (224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Bgzwy6C5dMM3"
      },
      "outputs": [],
      "source": [
        "if (len(train_folder_list)%batch_size) == 0:\n",
        "    steps_per_epoch = int(len(train_folder_list)/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = int(len(train_folder_list)//batch_size) + 1\n",
        "\n",
        "if (len(val_folder_list)%batch_size) == 0:\n",
        "    validation_steps = int(len(val_folder_list)/batch_size)\n",
        "else:\n",
        "    validation_steps = int(len(val_folder_list)//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(150, 10, 0, 15)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_folder_list), batch_size, len(train_folder_list)%batch_size, steps_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50, 10, 0, 5)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_folder_list), batch_size, len(val_folder_list)%batch_size, validation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Wx15tL1lf69E"
      },
      "outputs": [],
      "source": [
        "train_generator = run_generator(f'{data_folder}/train', train_folder_list, batch_size, image_shape)\n",
        "val_generator = run_generator(f'{data_folder}/val', val_folder_list, batch_size, image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-38sWtpzoe9l"
      },
      "outputs": [],
      "source": [
        "  # model = Sequential()\n",
        "\n",
        "  # model.add(layers.Conv3D(64, (3, 3, 3), activation='relu', input_shape=vid_shape)),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(1, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "\n",
        "  # model.add(layers.Conv3D(128, (3, 3, 3), activation='relu')),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(1, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "\n",
        "  # model.add(layers.Conv3D(256, (3, 3, 3), activation='relu')),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(1, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "\n",
        "  # model.add(layers.Conv3D(512, (3, 3, 3), activation='relu')),\n",
        "\n",
        "  # ## Flatten and Dense layers\n",
        "  # model.add(layers.GlobalAveragePooling3D()),\n",
        "  # model.add(layers.Dense(1024, activation='relu')),\n",
        "  # model.add(layers.Dense(1024, activation='relu')),\n",
        "  # model.add(layers.Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_classes = train_df['class'].unique().size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GfIWQS0ghS-y"
      },
      "outputs": [],
      "source": [
        "def cnn3D_model(vid_shape):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(layers.Input(shape=vid_shape)),\n",
        "  model.add(layers.Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')),\n",
        "  model.add(layers.MaxPooling3D(pool_size=(2, 2, 2))),\n",
        "  model.add(layers.BatchNormalization(axis=-1)),\n",
        "\n",
        "  model.add(layers.Conv3D(filters=64, kernel_size=(3, 3, 3), activation='relu')),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(2, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "  \n",
        "  # model.add(layers.Conv3D(filters=128, kernel_size=(3, 3, 3), activation='relu')),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(2, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "  \n",
        "  # model.add(layers.Conv3D(filters=256, kernel_size=(3, 3, 3), activation='relu')),\n",
        "  # model.add(layers.MaxPooling3D(pool_size=(2, 2, 2))),\n",
        "  # model.add(layers.BatchNormalization()),\n",
        "  \n",
        "  # model.add(layers.Conv3D(filters=512, kernel_size=(3, 3, 3), activation='relu')),\n",
        "\n",
        "  ## Flatten and Dense layers\n",
        "  model.add(layers.GlobalAveragePooling3D()),\n",
        "  # model.add(layers.Flatten()),\n",
        "  model.add(layers.Dense(512, activation='relu')),\n",
        "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "8soey6k0vV_b",
        "outputId": "dea1f6ea-4aed-4d39-b7e6-499b9a19df98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">222</span>,   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,248</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>,    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>,    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">109</span>,    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,656</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling3d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling3D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,280</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m222\u001b[0m, \u001b[38;5;34m222\u001b[0m,   │         \u001b[38;5;34m5,248\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m64\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m,    │             \u001b[38;5;34m0\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m64\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m111\u001b[0m,    │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │ \u001b[38;5;34m64\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m109\u001b[0m, \u001b[38;5;34m109\u001b[0m,    │       \u001b[38;5;34m110,656\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m64\u001b[0m)                    │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling3d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling3D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m33,280\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m2,565\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">152,005</span> (593.77 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m152,005\u001b[0m (593.77 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,877</span> (593.27 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m151,877\u001b[0m (593.27 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = cnn3D_model((15, 224, 224, 3))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFWQ9YyXWt1P",
        "outputId": "6c7e5198-37f2-4346-c79f-dfc8c91e759c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 7s/step - categorical_accuracy: 0.2601 - loss: 1.5843 - val_categorical_accuracy: 0.2300 - val_loss: 1.5983\n",
            "Epoch 2/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 15s/step - categorical_accuracy: 0.3701 - loss: 1.4714 - val_categorical_accuracy: 0.2200 - val_loss: 1.6616\n",
            "Epoch 3/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step - categorical_accuracy: 0.4447 - loss: 1.2723 - val_categorical_accuracy: 0.2000 - val_loss: 1.7086\n",
            "Epoch 4/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.5622 - loss: 1.0591 - val_categorical_accuracy: 0.2300 - val_loss: 1.6150\n",
            "Epoch 5/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.5225 - loss: 1.1304 - val_categorical_accuracy: 0.4100 - val_loss: 1.5203\n",
            "Epoch 6/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5s/step - categorical_accuracy: 0.5923 - loss: 0.9728 - val_categorical_accuracy: 0.4300 - val_loss: 1.4701\n",
            "Epoch 7/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5s/step - categorical_accuracy: 0.6230 - loss: 0.9234 - val_categorical_accuracy: 0.3400 - val_loss: 1.5223\n",
            "Epoch 8/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 5s/step - categorical_accuracy: 0.7068 - loss: 0.7817 - val_categorical_accuracy: 0.1400 - val_loss: 1.7210\n",
            "Epoch 9/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.6818 - loss: 0.7456 - val_categorical_accuracy: 0.2200 - val_loss: 1.5043\n",
            "Epoch 10/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m909s\u001b[0m 28s/step - categorical_accuracy: 0.6849 - loss: 0.7791 - val_categorical_accuracy: 0.2000 - val_loss: 1.7830\n",
            "Epoch 11/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 5s/step - categorical_accuracy: 0.6629 - loss: 0.7957 - val_categorical_accuracy: 0.2600 - val_loss: 1.7247\n",
            "Epoch 12/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.6635 - loss: 0.7875 - val_categorical_accuracy: 0.2400 - val_loss: 1.5422\n",
            "Epoch 13/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 5s/step - categorical_accuracy: 0.7409 - loss: 0.7206 - val_categorical_accuracy: 0.3800 - val_loss: 1.4371\n",
            "Epoch 14/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 14s/step - categorical_accuracy: 0.7068 - loss: 0.7230 - val_categorical_accuracy: 0.2100 - val_loss: 1.8887\n",
            "Epoch 15/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 7s/step - categorical_accuracy: 0.7548 - loss: 0.5830 - val_categorical_accuracy: 0.4400 - val_loss: 1.7465\n",
            "Epoch 16/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 5s/step - categorical_accuracy: 0.7599 - loss: 0.5941 - val_categorical_accuracy: 0.3600 - val_loss: 2.1884\n",
            "Epoch 17/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.7617 - loss: 0.6048 - val_categorical_accuracy: 0.5000 - val_loss: 1.0862\n",
            "Epoch 18/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5s/step - categorical_accuracy: 0.8078 - loss: 0.5582 - val_categorical_accuracy: 0.3000 - val_loss: 4.8575\n",
            "Epoch 19/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 7s/step - categorical_accuracy: 0.7914 - loss: 0.5530 - val_categorical_accuracy: 0.2300 - val_loss: 3.9448\n",
            "Epoch 20/20\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step - categorical_accuracy: 0.7970 - loss: 0.5159 - val_categorical_accuracy: 0.2321 - val_loss: 4.4755\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x1e128698da0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK7pZuHnWtyt"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP7lD8l2WtwF"
      },
      "outputs": [],
      "source": [
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6BtMX9MDm7v"
      },
      "outputs": [],
      "source": [
        "train_generator = run_generator(f'{data_folder}/train', train_folder_list, batch_size, image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(next(train_generator)[0].shape, next(train_generator)[1].shape)\n",
        "# print(next(train_generator)[0], next(train_generator)[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_generator = run_generator(f'{data_folder}/val', val_folder_list, batch_size, image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(next(val_generator)[0].shape, next(val_generator)[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(30):\n",
        "    print(next(train_generator)[0].shape, next(train_generator)[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_generator = run_generator(f'{data_folder}/val', val_folder_list, 4, image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "j = 0\n",
        "for i in range(30):\n",
        "    # print(next(val_generator)[0].shape, next(val_generator)[1].shape)\n",
        "    if j%3 == 0:\n",
        "        # print(next(val_generator)[1], end = '\\n\\n')\n",
        "        print(next(val_generator)[0].shape, next(val_generator)[1].shape)\n",
        "        # print(next(val_generator))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_folder_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "folder_list = [\"WIN_20180925_18_01_40_Pro_Left_Swipe_new;Left_Swipe_new;0\\n\",\n",
        "               \"WIN_20180907_16_02_09_Pro_Thumbs Down_new;Thumbs Down_new;3\\n\",\n",
        "               \"WIN_20180907_16_05_10_Pro_Right Swipe_new;Right Swipe_new;1\\n\",\n",
        "               \"WIN_20180907_16_18_23_Pro_Thumbs Down_new;Thumbs Down_new;3\\n\",\n",
        "               \"WIN_20180925_17_38_43_Pro_Thumbs_Up_new;Thumbs_Up_new;4\\n\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "## path => folder path as string\n",
        "## folder_list => list of folders in train/test folder as list\n",
        "## batch_size => chosen batch size as int\n",
        "## image_shape => as tuple (image_height, image_width)\n",
        "def run_generator_1(path, folder_list, batch_size, image_shape):\n",
        "  num_batches, folders_remain = get_batch_details(folder_list, batch_size)\n",
        "  ## set how many images to view from total images of the video\n",
        "  img_ids = list(range(0,30,2))\n",
        "  while True:\n",
        "    folder_list = np.random.permutation(folder_list)\n",
        "    for batch in range(num_batches):\n",
        "      # batch_images, batch_labels = create_batch_folders(batch_size, img_ids, image_shape)\n",
        "      batch_images = np.zeros(shape = (batch_size, len(img_ids), image_shape[0], image_shape[1], 3))\n",
        "      batch_labels = np.zeros(shape = (batch_size, 5))\n",
        "      ## get folders for that batch\n",
        "      for folder in range(batch_size):\n",
        "        # process_images(folder, f'{path}/{folder_list[folder + (batch * batch_size)]}', img_ids, image_shape, batch_images)\n",
        "      ## get images and labels for each folder in the batch\n",
        "        imgs = os.listdir(f'{path}/{folder_list[folder + (batch * batch_size)].split(\";\")[0]}')\n",
        "        for idx, ind in enumerate(img_ids):\n",
        "            ## read image\n",
        "            image = cv2.imread(f'{path}/{folder_list[folder + (batch * batch_size)].split(\";\")[0]}/{imgs[ind]}').astype(np.float32)\n",
        "            ## resize image\n",
        "            image = cv2.resize(image, (image_shape[0], image_shape[1]), interpolation = cv2.INTER_LINEAR)\n",
        "            ## Seperating them into their respective R, G, B channels and normalizing the values\n",
        "            ## Load labels for each image\n",
        "            batch_images[folder, idx, :, :, 0] = image[:,:,0]/255.0   ## R\n",
        "            batch_images[folder, idx, :, :, 1] = image[:,:,1]/255.0   ## G\n",
        "            batch_images[folder, idx, :, :, 2] = image[:,:,2]/255.0   ## B\n",
        "          ## Load labels for each image\n",
        "        batch_labels[folder, int(folder_list[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
        "      yield batch_images, batch_labels\n",
        "      \n",
        "    ## Process the remaining folders\n",
        "    # batch_images, batch_labels = create_batch_folders(folders_remain, img_ids, image_shape)\n",
        "    if folders_remain > 0:\n",
        "      batch_images = np.zeros(shape = (folders_remain, len(img_ids), image_shape[0], image_shape[1], 3))\n",
        "      batch_labels = np.zeros(shape = (folders_remain, 5))\n",
        "      folder_rem = folder_list[-folders_remain:]\n",
        "      for folder in range(folders_remain):\n",
        "      #   process_images(folder, f'{path}/{folder_rem[folder]}', img_ids, image_shape, batch_images)\n",
        "        imgs = os.listdir(f'{path}/{folder_rem[folder].split(\";\")[0]}')\n",
        "        for idx, ind in enumerate(img_ids):\n",
        "            ## read image\n",
        "            image = cv2.imread(f'{path}/{folder_rem[folder].split(\";\")[0]}/{imgs[ind]}').astype(np.float32)\n",
        "            ## resize image\n",
        "            image = cv2.resize(image, (image_shape[0], image_shape[1]), interpolation = cv2.INTER_LINEAR)\n",
        "            ## Seperating them into their respective R, G, B channels and normalizing the values\n",
        "            ## Load labels for each image\n",
        "            batch_images[folder, idx, :, :, 0] = image[:,:,0]/255.0   ## R\n",
        "            batch_images[folder, idx, :, :, 1] = image[:,:,1]/255.0   ## G\n",
        "            batch_images[folder, idx, :, :, 2] = image[:,:,2]/255.0   ## B\n",
        "        batch_labels[folder, int(folder_rem[folder].strip().split(';')[2])] = 1\n",
        "      print(batch_images.shape, batch_labels.shape)\n",
        "      yield batch_images, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_folder_list = train_folder_list_[:150]\n",
        "val_folder_list = val_folder_list_[-15:]\n",
        "\n",
        "batch_size = 5\n",
        "num_epochs = 20\n",
        "image_shape = (224, 224, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_generator = run_generator_1(f'{data_folder}/val', val_folder_list, 5, image_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# j = 0\n",
        "# for i in range(4):\n",
        "#     # print(next(val_generator)[1], end = '\\n\\n')\n",
        "#     # print(next(val_generator)[0].shape, next(val_generator)[1].shape)\n",
        "#     next(val_generator)\n",
        "\n",
        "next(val_generator)[0].shape, next(val_generator)[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "folders_remain = 3\n",
        "\n",
        "folder_list[-folders_remain:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "###        Build a Model with CNN-GRU network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gru_model(vid_shape):\n",
        "    resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Create a sequential model to process the video data\n",
        "    model = Sequential()\n",
        "\n",
        "    # Use TimeDistributed to apply ResNet50 to each frame of the video\n",
        "    model.add(TimeDistributed(resnet_model, input_shape=(15, 224, 224, 3)))\n",
        "\n",
        "    # You can add a global pooling layer after ResNet50 (optional)\n",
        "    model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "\n",
        "    # Add a GRU layer to process the sequence of features from each frame\n",
        "    model.add(GRU(256))\n",
        "\n",
        "    # Add a dense layer for classification (optional, depending on your task)\n",
        "    model.add(Dense(5, activation='softmax'))  # Assuming 5 classes\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = gru_model((224, 224, 3))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.fit(\n",
        "            train_generator, \n",
        "            steps_per_epoch = steps_per_epoch, \n",
        "            epochs = num_epochs, \n",
        "            verbose = 1,\n",
        "            validation_data = val_generator,\n",
        "            validation_steps = validation_steps, \n",
        "            class_weight = None, \n",
        "            initial_epoch = 0\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pkhli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">42,658,176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,771,008</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m) │    \u001b[38;5;34m42,658,176\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_3 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,771,008\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m1,285\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,430,469</span> (169.49 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,430,469\u001b[0m (169.49 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,772,293</span> (6.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,772,293\u001b[0m (6.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,658,176</span> (162.73 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,658,176\u001b[0m (162.73 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_classes = 5  # Adjust according to your dataset\n",
        "\n",
        "# Input shape: (timesteps, height, width, channels)\n",
        "input_shape = (15, 224, 224, 3)  # 15 frames per sequence\n",
        "\n",
        "# Load ResNet101 as the feature extractor\n",
        "resnet101 = ResNet101(weights='imagenet', include_top=False)\n",
        "\n",
        "# Freeze ResNet101 layers to prevent training\n",
        "resnet101.trainable = False\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    # TimeDistributed applies ResNet101 to each frame\n",
        "    TimeDistributed(resnet101, input_shape=input_shape),\n",
        "    TimeDistributed(GlobalAveragePooling2D()),  # Pooling spatial features\n",
        "    GRU(256, return_sequences=False),  # GRU for temporal modeling\n",
        "    Dropout(0.5),  # Regularization\n",
        "    Dense(num_classes, activation='softmax')  # Output layer for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 18s/step - accuracy: 0.2171 - loss: 1.8786 - val_accuracy: 0.2200 - val_loss: 1.5833\n",
            "Epoch 2/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 12s/step - accuracy: 0.2294 - loss: 1.7060 - val_accuracy: 0.2200 - val_loss: 1.5748\n",
            "Epoch 3/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 12s/step - accuracy: 0.2086 - loss: 1.7955 - val_accuracy: 0.2000 - val_loss: 1.5808\n",
            "Epoch 4/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 12s/step - accuracy: 0.1978 - loss: 1.7951 - val_accuracy: 0.2400 - val_loss: 1.5679\n",
            "Epoch 5/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 12s/step - accuracy: 0.2420 - loss: 1.6922 - val_accuracy: 0.2200 - val_loss: 1.5524\n",
            "Epoch 6/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 12s/step - accuracy: 0.2089 - loss: 1.6645 - val_accuracy: 0.3200 - val_loss: 1.5326\n",
            "Epoch 7/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 12s/step - accuracy: 0.2695 - loss: 1.6557 - val_accuracy: 0.3000 - val_loss: 1.5132\n",
            "Epoch 8/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 12s/step - accuracy: 0.2303 - loss: 1.6797 - val_accuracy: 0.2800 - val_loss: 1.5155\n",
            "Epoch 9/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 12s/step - accuracy: 0.2786 - loss: 1.6256 - val_accuracy: 0.3400 - val_loss: 1.4950\n",
            "Epoch 10/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 12s/step - accuracy: 0.3278 - loss: 1.5586 - val_accuracy: 0.2200 - val_loss: 1.4763\n",
            "Epoch 11/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 12s/step - accuracy: 0.2943 - loss: 1.5352 - val_accuracy: 0.4200 - val_loss: 1.4794\n",
            "Epoch 12/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 12s/step - accuracy: 0.3505 - loss: 1.5195 - val_accuracy: 0.2200 - val_loss: 1.4596\n",
            "Epoch 13/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 12s/step - accuracy: 0.2856 - loss: 1.5078 - val_accuracy: 0.4200 - val_loss: 1.4390\n",
            "Epoch 14/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 12s/step - accuracy: 0.3295 - loss: 1.5161 - val_accuracy: 0.3600 - val_loss: 1.4314\n",
            "Epoch 15/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 12s/step - accuracy: 0.3195 - loss: 1.4736 - val_accuracy: 0.3800 - val_loss: 1.4310\n",
            "Epoch 16/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 12s/step - accuracy: 0.3351 - loss: 1.5109 - val_accuracy: 0.4000 - val_loss: 1.4069\n",
            "Epoch 17/20\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 12s/step - accuracy: 0.3694 - loss: 1.4079 - val_accuracy: 0.4000 - val_loss: 1.3965\n",
            "Epoch 18/20\n",
            "\u001b[1m10/15\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 9s/step - accuracy: 0.3840 - loss: 1.4668"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "model.fit(\n",
        "            train_generator, \n",
        "            steps_per_epoch = steps_per_epoch, \n",
        "            epochs = num_epochs, \n",
        "            verbose = 1,\n",
        "            validation_data = val_generator,\n",
        "            validation_steps = validation_steps, \n",
        "            class_weight = None, \n",
        "            initial_epoch = 0\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\pkhli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <TimeDistributed name=time_distributed_7, built=False> (of type <class 'keras.src.layers.rnn.time_distributed.TimeDistributed'>)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m resnet_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m70\u001b[39m:]: layer\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     29\u001b[0m resnet_output \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mTimeDistributed(resnet_model, input_shape \u001b[38;5;241m=\u001b[39m vid_input)\n\u001b[1;32m---> 30\u001b[0m resnet_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m gru_model \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mGRU(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(resnet_output)\n\u001b[0;32m     32\u001b[0m gru_model \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mTimeDistributed(layers\u001b[38;5;241m.\u001b[39mDense(num_classes, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))(gru_model)\n",
            "File \u001b[1;32mc:\\Users\\pkhli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\pkhli\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\layer.py:808\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mflatten(args):\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    804\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, KerasTensor)\n\u001b[0;32m    805\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mis_tensor(arg)\n\u001b[0;32m    806\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         ):\n\u001b[1;32m--> 808\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    809\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly input tensors may be passed as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    810\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional arguments. The following argument value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    811\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould be passed as a keyword argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m             )\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# Caches info about `call()` signature, args, kwargs.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m call_spec \u001b[38;5;241m=\u001b[39m CallSpec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_signature, args, kwargs)\n",
            "\u001b[1;31mValueError\u001b[0m: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <TimeDistributed name=time_distributed_7, built=False> (of type <class 'keras.src.layers.rnn.time_distributed.TimeDistributed'>)"
          ]
        }
      ],
      "source": [
        "# vid_input = layers.Input(shape=(15, 224, 224, 3))\n",
        "# resnet_model = ResNet101(\n",
        "#                             include_top = False,\n",
        "#                             weights = 'imagenet',\n",
        "#                             pooling = None,\n",
        "#                             input_tensor=layers.Input(shape=vid_input)\n",
        "#                         )\n",
        "# for layer in resnet_model.layers[:70]: layer.trainable = False\n",
        "# for layer in resnet_model.layers[70:]: layer.trainable = True\n",
        "# resnet_output = layers.GlobalAveragePooling2D()(resnet_model.output)\n",
        "# cnn_model = keras.Model(inputs = resnet_model.input, outputs = resnet_output)\n",
        "# cnn_model = layers.TimeDistributed(cnn_model)(vid_input)\n",
        "\n",
        "# gru_model = layers.GRU(units = 256, return_sequences=True)(cnn_model)\n",
        "# gru_model = layers.TimeDistributed(layers.Dense(num_classes, activation='softmax'))(gru_model)\n",
        "\n",
        "\n",
        "\n",
        "# vid_input = (15, 224, 224, 3)\n",
        "# resnet_model = ResNet101(\n",
        "#                             include_top = False,\n",
        "#                             weights = 'imagenet',\n",
        "#                             pooling = None,\n",
        "#                             input_shape=image_shape\n",
        "#                         )\n",
        "# for layer in resnet_model.layers[:70]: layer.trainable = False\n",
        "# for layer in resnet_model.layers[70:]: layer.trainable = True\n",
        "\n",
        "# resnet_output = layers.TimeDistributed(resnet_model, input_shape = vid_input)\n",
        "# resnet_output = layers.TimeDistributed(layers.GlobalAveragePooling2D())(resnet_output)\n",
        "# gru_model = layers.GRU(units = 256, return_sequences=True)(resnet_output)\n",
        "# gru_model = layers.TimeDistributed(layers.Dense(num_classes, activation='softmax'))(gru_model)\n",
        "\n",
        "# grumodel = keras.Model(inputs = resnet_model.input, outputs = gru_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gru_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    resnet_model = ResNet101(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        pooling=None,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    for layer in resnet_model.layers[:65]:\n",
        "        layer.trainable = False\n",
        "    for layer in resnet_model.layers[65:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.add(layers.TimeDistributed(resnet_model, input_shape = (15, 224, 224, 3))),\n",
        "    model.add(layers.TimeDistributed(layers.GlobalAveragePooling2D())),\n",
        "    model.add(layers.GRU(units=64, return_sequences=False)),\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def gru_model():\n",
        "#     model = Sequential()\n",
        "#     model.add(layers.Input(shape=(15, 224, 224, 3))),\n",
        "\n",
        "#     resnet_model = ResNet101(\n",
        "#                                 include_top = False,\n",
        "#                                 weights = 'imagenet',\n",
        "#                                 pooling = None,\n",
        "#                                 input_shape = (224, 224, 3)\n",
        "#                              )\n",
        "#     for layer in resnet_model.layers[:65]: layer.trainable = False\n",
        "#     for layer in resnet_model.layers[65:]: layer.trainable = True\n",
        "\n",
        "#     model.add(layers.TimeDistributed(resnet_model)),\n",
        "#     model.add(layers.TimeDistributed(layers.GlobalAveragePooling2D())),\n",
        "#     model.add(layers.GRU(units = 256, return_sequences=True))\n",
        "#     model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#     return resnet_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gru_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    resnet_model = ResNet101(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        pooling=None,\n",
        "        input_shape=(224, 224, 3)\n",
        "    )\n",
        "\n",
        "    for layer in resnet_model.layers[:65]:\n",
        "        layer.trainable = False\n",
        "    for layer in resnet_model.layers[65:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    model.add(layers.TimeDistributed(resnet_model, input_shape = (15, 224, 224, 3))),\n",
        "    model.add(layers.TimeDistributed(layers.GlobalAveragePooling2D())),\n",
        "    model.add(layers.GRU(units=2, return_sequences=False)),\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">42,658,176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m) │    \u001b[38;5;34m42,658,176\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m2048\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gru_2 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │        \u001b[38;5;34m12,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m15\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,670,503</span> (162.78 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m42,670,503\u001b[0m (162.78 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">41,463,335</span> (158.17 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m41,463,335\u001b[0m (158.17 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,207,168</span> (4.60 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,207,168\u001b[0m (4.60 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = gru_model()                                                                                  # keras.Model(resnet_model.input, outputs = gru_model)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "model.fit(\n",
        "            train_generator, \n",
        "            steps_per_epoch = steps_per_epoch, \n",
        "            epochs = num_epochs, \n",
        "            verbose = 1,\n",
        "            validation_data = val_generator,\n",
        "            validation_steps = validation_steps, \n",
        "            class_weight = None, \n",
        "            initial_epoch = 0\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
